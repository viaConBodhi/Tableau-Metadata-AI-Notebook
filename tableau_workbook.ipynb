{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/tableau_meta.png\" alt=\"Tableau Meta\" width=\"300\" height=\"300\" style=\"float:left; margin-right: 40px; margin-bottom: 20px;\" />\n",
    "\n",
    "# Tableau Metadata AI Notebook\n",
    "* #### This notebook provides methods in Python to leverage Tableau's APIs to:\n",
    "1. Extract details for Tableau server workbooks and data sources/fields.\n",
    "2. Parse Tableau workbook metadata/XML enabling AI translations to describe workbooks.\n",
    "3. Utilize AI meta translation to build documentation resources.\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "- [Prerequisites](#prerequisite)\n",
    "   - [OpenAI API Key](#openai-api-key) \n",
    "   - [Tableau Server (optional)](#tableau-server-optional) \n",
    "1. [Step 1 Set Up](#step-1-set-up)\n",
    "2. [Step 2 Testing API Connections](#step-2-testing-api-connections)\n",
    "   - [OpenAI API](#openai-api)\n",
    "   - [Tableau Server API (optional)](#tableau-server-api-optional) \n",
    "3. [Step 3 Walk Through](#step-3-walk-through)\n",
    "   - [3.01 Get All Datasources](#301-get-all-datasources) \n",
    "   - [3.02 Get All Workbooks](#302-get-all-workbooks)  \n",
    "   - [3.03 Download Workbook and Parse XML](#303-download-workbook-and-parse-xml)  \n",
    "   - [3.04 Get Worksheet Meta](#304-get-worksheet-meta)\n",
    "   - [3.05 Get Dashboard Meta](#305-get-dashboard-meta) \n",
    "   - [3.06 Get Worksheets in Dashboard](#306-get-worksheets-in-dashboard)\n",
    "   - [3.07 Get XML](#307-get-xml)\n",
    "   - [3.08 Helper Functions](#308-helper-functions)\n",
    "   - [3.09 Preprocessing](#309-preprocessing)\n",
    "   - [3.10 AI Processing](#310-ai-preprocessing)\n",
    "\n",
    "\n",
    "\n",
    "For details on Tableau API methods, see the [Tableau REST API Reference](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api.htm).\n",
    "\n",
    "For details on OpenAI API methods, see the [OpenAI API Reference](https://platform.openai.com/docs/api-reference/introduction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"prerequisites\">Prerequisites</h1> <a id=\"prerequisites\"></a>\n",
    "\n",
    "<h2 id=\"openai-api-key\">OpenAI API Key</h2> <a id=\"openai-api-key\"></a> \n",
    "To use the notebook as currently configured, you will need an API key from OpenAI. However, with minor adjustments to the codebase, you can theoretically use any generative AI tool. The codebase currently limits the token count for AI processing to 10,000 tokens. For more efficient results, it is recommended to use a model that can handle up to 100,000 tokens, allowing for processing larger inputs or generating longer outputs.\n",
    "\n",
    "\n",
    "<h2 id=\"tableau-server-optional\">Tableau Server (optional)</h2> <a id=\"tableau-server-optional\"></a>\n",
    "This notebook includes operations to support data extractions from the server, but Tableau workbooks can also be processed locally by adjusting the file paths within the codebase. If you are using a Tableau server, please note that your access to products and data sources may be limited based on your site permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"step-1-set-up\">Step 1 Set Up</h1> <a id=\"step-1-set-up\"></a>\n",
    "\n",
    "#### Steps to Setup\n",
    "1. Create a virtual environment for the project\n",
    "2. Start the notebook in that virtual environment\n",
    "3. Uncomment the line { !pip install -r requirements.txt } and run the cell to install the required packages then comment that line out when completed\n",
    "4. Import the packages into the notebook\n",
    "\n",
    "Note: This assumes youâ€™re running the notebook from a local machine and not a shared resource like CoLab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tableauserverclient as TSC\n",
    "from tableauserverclient import TableauAuth, Server, RequestOptions\n",
    "from tableaudocumentapi import Workbook\n",
    "from tableaudocumentapi.xfile import xml_open\n",
    "import lxml.etree as etree\n",
    "import openai  # for using GPT and getting embeddings\n",
    "import tiktoken # counting tokens, used by openai \n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"step-2-testing-api-connections\">Step 2 Testing API Connections</h1> <a id=\"step-2-testing-api-connections\"></a>\n",
    "\n",
    "#### Steps to Test Connections\n",
    "1. OpenAI and Tableau connections will be tested separately\n",
    "2. Update the respective variables for each connection\n",
    "3. Run the respective cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"openai-api\">OpenAI API</h2> <a id=\"openai-api\"></a>\n",
    "\n",
    "#### Testing your OpenAI API connection\n",
    "- Update the API key and AI model (if needed) then run the respective cells\n",
    "- The code should print the error response allowing you to troubleshoot. Expected issues would be either your access to particular models or issues with your API key permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Action**: Update the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the following variables \n",
    "model = \"gpt-4-1106-preview\"\n",
    "api_key = 'YOUR KEY HERE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Action**: Run the following test cell to determine if there are issues with your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up your OpenAI client\n",
    "openai.api_key = api_key\n",
    "\n",
    "# A test prompt to send to the OpenAI API\n",
    "test_prompt = \"Does this work?\"\n",
    "\n",
    "# API endpoint URL\n",
    "api_url = 'https://api.openai.com/v1/chat/completions'\n",
    "\n",
    "# Request headers\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {api_key}'\n",
    "}\n",
    "\n",
    "\n",
    "data = {\n",
    "    'model': model,\n",
    "    'messages': [{'role': 'system', 'content': \"You are a helpful assistant.\"},\n",
    "                    {'role': 'user', 'content': test_prompt}]\n",
    "}\n",
    "\n",
    "# Make the API call\n",
    "response = requests.post(api_url, json=data, headers=headers)\n",
    "\n",
    "# Process the API response\n",
    "if response.status_code == 200:\n",
    "    print('success')\n",
    "    result = response.json()\n",
    "    chatgpt_response = result['choices'][0]['message']['content']\n",
    "\n",
    "else:\n",
    "    print('error')\n",
    "    chatgpt_response = f\"'Error:', {response.status_code}, {response.text}\"\n",
    "print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tableau-server-api-optional\">Tableau Server API (optional)</h2> <a id=\"tableau-server-api-optional\"></a>\n",
    "\n",
    "#### Testing your Tableau Server API connection\n",
    "- Update the login variables then run the respective cells\n",
    "- The code should print the error response allowing you to troubleshoot. Expected issues would be either your credentials or if your server requires you to be on a network.\n",
    "- This is listed as optional as you can use a Tableau workbook on your local machine by replacing \"file_path\" with the path to your file and and running that line of code down to the end of the cell by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Action**: Update the following variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the following variables \n",
    "user_name = 'USER NAME'\n",
    "pw = 'PASSWORD'\n",
    "site_name = 'SITE-NAME'\n",
    "server = 'https://YOUR-SERVER.name'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Action**: Run the following test cell to determine if there are issues with your Tableau API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tableau_auth = TSC.TableauAuth(user_name, pw, site_name)\n",
    "server = TSC.Server(server, use_server_version=True)\n",
    "# server.version = '3.7'\n",
    "\n",
    "try:\n",
    "    with server.auth.sign_in(tableau_auth):\n",
    "        # Retrieve server information\n",
    "        server_info = server.server_info.get()\n",
    "        print(\"Successfully connected to the server!\")\n",
    "        print(\"Server version:\", server_info.product_version)\n",
    "\n",
    "        # Optionally, list available projects to verify access permissions\n",
    "        all_projects, pagination_item = server.projects.get()\n",
    "        print(\"\\nProjects accessible to you:\")\n",
    "        for project in all_projects:\n",
    "            print(\"-\", project.name)\n",
    "except TSC.ServerResponseError as e:\n",
    "    print(\"Error connecting to the server:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"step-3-walk-through\">Step 3 Walk Through</h1> <a id=\"step-3-walk-through\"></a>\n",
    "  \n",
    "\n",
    "<h2 id=\"301-get-all-datasources\">3.01 Get All Datasources</h2> <a id=\"301-get-all-datasources\"></a>\n",
    "\n",
    "##### Get all datasources from your Tableau site to include source IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_sources_details = []\n",
    "\n",
    "with server.auth.sign_in(tableau_auth):\n",
    "    all_datasources, pagination_item = server.datasources.get()\n",
    "    print(\"\\nThere are {} datasources on site: \".format(pagination_item.total_available))\n",
    "\n",
    "    for datasource in all_datasources:\n",
    "        # Populate connections for the data source\n",
    "        server.datasources.populate_connections(datasource)\n",
    "\n",
    "        for connection in datasource.connections:\n",
    "            # Append details to the list\n",
    "            data_sources_details.append({\n",
    "                \"Data Source Name\": datasource.name,\n",
    "                \"Data Source ID\": datasource.id,\n",
    "                \"Connection Type\": getattr(connection, 'connection_type', 'N/A'),\n",
    "                \"Connection ID\": connection.id\n",
    "            })\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "tableau_data_sources = pd.DataFrame(data_sources_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"302-get-all-workbooks\">3.02 Get All Workbooks</h2> <a id=\"302-get-all-workbooks\"></a>\n",
    "\n",
    "##### Get all workbooks from your Tableau site with helpful meta...this one could take some time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sign in to the Tableau Server\n",
    "with server.auth.sign_in(tableau_auth):\n",
    "    workbooks_info = []  # To store workbook details\n",
    "\n",
    "    # Use TSC.Pager to handle pagination automatically\n",
    "    pager = TSC.Pager(server.workbooks)  # Automatically handles pagination\n",
    "\n",
    "    for workbook in pager:\n",
    "        try:\n",
    "            # Populate additional details for each workbook\n",
    "            server.workbooks.populate_connections(workbook)\n",
    "            server.workbooks.populate_views(workbook)\n",
    "\n",
    "            # Extract the required details\n",
    "            workbook_details = {\n",
    "                'workbook_name': workbook.name,\n",
    "                'id': workbook.id,\n",
    "                'project_name': workbook.project_name,\n",
    "                'project_id': workbook.project_id,\n",
    "                'connections': [connection.id for connection in workbook.connections],\n",
    "                'wookbook_tags': list(workbook.tags),  # Convert to list of strings\n",
    "                'workbook_views': [view.name for view in workbook.views],\n",
    "                'webpage_url': workbook.webpage_url\n",
    "            }\n",
    "            workbooks_info.append(workbook_details)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Issue with workbook {workbook.id}: {str(e)}\")\n",
    "\n",
    "    # Create a DataFrame from the workbook info\n",
    "    workbooks_df = pd.DataFrame(workbooks_info)\n",
    "\n",
    "\n",
    "    print(f\"Total workbooks processed: {len(workbooks_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"303-download-workbook-and-parse-xml\">3.03 Download Workbook and Parse XML</h2> <a id=\"303-download-workbook-and-parse-xml\"></a>\n",
    "\n",
    "##### Get your target workbook from the server and process the XML. Note: os.remove(file_path) has been commented out so update as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Action**: Update the variable { target_workbook } in the next cell with the title of workbook you would like to use for testing this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_workbook = 'YOUR WORKBOOK NAME HERE'\n",
    "\n",
    "find = workbooks_df[workbooks_df.workbook_name == target_workbook] \n",
    "find_views = find.iloc[0]['workbook_views']\n",
    "\n",
    "# get your workbook id and project id to extract the required workbook\n",
    "project_id = find.iloc[0]['project_id'] \n",
    "workbook_id = find.iloc[0]['id'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Action**: Run the next cell to download your workbook from the server and parse the XML meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download your workbook from the server and parse the XML meta \n",
    "\n",
    "def get_workbook_details(workbook_id, project_id, server, tableau_auth):   \n",
    "    with server.auth.sign_in(tableau_auth):\n",
    "        for workbook in TSC.Pager(server.workbooks):\n",
    "            # Match the workbook based on the workbook id and project id\n",
    "            if workbook.id == workbook_id and workbook.project_id == project_id:\n",
    "                return {\n",
    "                    \"Workbook Id\": workbook.id,\n",
    "                    \"Project Id\": workbook.project_id,\n",
    "                    \"Workbook Webpage_Url\": workbook.webpage_url,\n",
    "                    \"Workbook Name\": workbook.name\n",
    "                }\n",
    "        \n",
    "        print(\"No workbook found with the specified workbook ID and project ID.\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Locate workbook\n",
    "source_workbook = get_workbook_details(workbook_id, project_id, server, tableau_auth)\n",
    " \n",
    " \n",
    "with server.auth.sign_in(tableau_auth):    \n",
    "    file_path = server.workbooks.download(source_workbook[\"Workbook Id\"], filepath=None, include_extract=True, no_extract=None)\n",
    "    print(\"\\nDownloaded the file to {0}.\".format(file_path))\n",
    "\n",
    "    #*****Get list of active views and Dashboards from Worbook*****\n",
    "    views_to_include = []\n",
    " \n",
    "    workbook_item = server.workbooks.get_by_id(source_workbook[\"Workbook Id\"])\n",
    "    server.workbooks.populate_views(workbook_item)\n",
    "    for item in workbook_item.views:\n",
    "        views_to_include.append(item.name)\n",
    "\n",
    "\n",
    "\n",
    "#*****Get list of all views and Dashboards from Worbook***** \n",
    " \n",
    "#Load Workbook from Document API\n",
    "sourceWB = Workbook(file_path)\n",
    " \n",
    "#Get a list of all worksheets and dashboards\n",
    "all_views = sourceWB.dashboards + sourceWB.worksheets\n",
    " \n",
    "#Views not made visiable on the server\n",
    "views_to_hide = []\n",
    " \n",
    "for view in all_views:\n",
    "    if view not in views_to_include:\n",
    "        views_to_hide.append(view)\n",
    " \n",
    " \n",
    "#Remove downloaded file\n",
    "#os.remove(file_path)\n",
    " \n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(views_to_hide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"304-get-worksheet-meta\">3.04 Get Worksheet Meta</h2> <a id=\"304-get-worksheet-meta\"></a>\n",
    " \n",
    "##### Get the worksheets from the Tableau workbook in a dataframe with meta via fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Open the XML file\n",
    "xml_tree = xml_open(file_path)\n",
    "\n",
    "# Get the root element (the top-level element of the XML document)\n",
    "root = xml_tree.getroot()\n",
    "\n",
    "\n",
    "worksheets = root.findall(\".//worksheet\")\n",
    "\n",
    "df_worksheet_hold = []\n",
    "\n",
    "for worksheet in worksheets:\n",
    "    # Print the worksheet name\n",
    "    worksheet_name = worksheet.get('name')\n",
    "    #print(f\"Worksheet: {worksheet_name}\")\n",
    "    \n",
    "    # Find all 'datasource' elements within the 'datasources' tag under each worksheet\n",
    "    datasources = worksheet.findall(\".//datasources/datasource\")\n",
    "    \n",
    "    for datasource in datasources:\n",
    "        # Get the 'caption' or 'name' attribute of the datasource\n",
    "        caption = datasource.get('caption', 'No Caption')  # Use a default if 'caption' doesn't exist\n",
    "        name = datasource.get('name', 'No Name')  # Use a default if 'name' doesn't exist\n",
    "        #print(f\"  Datasource: Caption={caption}, Name={name}\")\n",
    "        \n",
    "        # Find columns in the datasource-dependencies for the current datasource\n",
    "        datasource_dependencies = worksheet.findall(f\".//datasource-dependencies[@datasource='{name}']/column\")\n",
    "\n",
    "\n",
    "        # Handle columns\n",
    "        for column in datasource_dependencies:\n",
    "            # Get column attributes\n",
    "            column_name =  re.sub('\\]|\\[','', column.get('name', 'No Name'))\n",
    "            column_caption = column.get('caption', 'No Caption')\n",
    "            column_datatype = column.get('datatype', 'No Datatype')\n",
    "            column_aggregation = column.get('aggregation', 'No Aggregation')\n",
    "            column_role = column.get('role', 'No Role')\n",
    "            column_formula = column.find(\".//calculation\").get('formula', 'No Formula') if column.find(\".//calculation\") is not None else 'No Formula'\n",
    "            \n",
    "            # Print the column details\n",
    "            #print(f\"    Column Name: {column_name}, Caption: {column_caption}, Datatype: {column_datatype}, Aggregation: {column_aggregation}, Role: {column_role}\")\n",
    "            #print(f\"    Formula: {column_formula}\")\n",
    "            data = {'type':'worksheet',\n",
    "                    'title':worksheet_name,\n",
    "                    'datasource':caption,\n",
    "                    'column_name':column_caption,\n",
    "                    'formula':column_formula}\n",
    "            df_temp1 = pd.DataFrame([data])\n",
    "            df_worksheet_hold.append(df_temp1)\n",
    "        \n",
    "        # Find column instances for the current datasource\n",
    "        column_instances = worksheet.findall(f\".//datasource-dependencies[@datasource='{name}']/column-instance\")\n",
    "\n",
    "\n",
    "        # Handle column instances\n",
    "        for column_instance in column_instances:\n",
    "            # Get column-instance attributes\n",
    "            column_instance_name = column_instance.get('name', 'No Name')\n",
    "            column_instance_base_column = re.sub('\\]|\\[','', column_instance.get('column', 'No Base Column'))\n",
    "            column_instance_derivation = column_instance.get('derivation', 'No Derivation')\n",
    "            column_instance_type = column_instance.get('type', 'No Type')\n",
    "\n",
    "            # Print the column-instance details\n",
    "            #print(f\"    Column Instance: {column_instance_name}, Base Column: {column_instance_base_column}, Derivation: {column_instance_derivation}, Type: {column_instance_type}\")\n",
    "\n",
    "            data = {'type':'worksheet',\n",
    "                    'title':worksheet_name,\n",
    "                    'datasource':caption,\n",
    "                    'column_name':column_instance_base_column,}\n",
    "            df_temp2 = pd.DataFrame([data])\n",
    "            df_worksheet_hold.append(df_temp2)\n",
    "\n",
    "\n",
    "df_worksheet_concat = pd.concat(df_worksheet_hold).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"305-get-dashboard-meta\">3.05 Get Dashboard Meta</h2> <a id=\"305-get-dashboard-meta\"></a>\n",
    "\n",
    "\n",
    "##### Get the dashboards from the Tableau workbook in a dataframe with meta on fields used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all dashboard elements\n",
    "dashboards = root.findall(\".//dashboard\")\n",
    "\n",
    "df_dashoard_hold = []\n",
    "\n",
    "for dashboard in dashboards:\n",
    "    # Print the dashboard name\n",
    "    dashboard_name = dashboard.get('name')\n",
    "    #print(f\"Dashboard: {dashboard_name}\")\n",
    "    \n",
    "    # Find all 'datasource' elements within the 'datasources' tag under each dashboard\n",
    "    datasources = dashboard.findall(\".//datasources/datasource\")\n",
    "    \n",
    "    for datasource in datasources:\n",
    "        # Get the 'caption' or 'name' attribute of the datasource\n",
    "        caption = datasource.get('caption', 'No Caption')  # Use a default if 'caption' doesn't exist\n",
    "        name = datasource.get('name', 'No Name')  # Use a default if 'name' doesn't exist\n",
    "        #print(f\"  Datasource: Caption={caption}, Name={name}\")\n",
    "        \n",
    "        # Find columns in the datasource-dependencies for the current datasource\n",
    "        datasource_dependencies = dashboard.findall(f\".//datasource-dependencies[@datasource='{name}']/column\")\n",
    "\n",
    "        # Handle columns\n",
    "        for column in datasource_dependencies:\n",
    "            # Get column attributes\n",
    "            column_name = re.sub(r'\\]|\\[', '', column.get('name', 'No Name'))\n",
    "            column_caption = column.get('caption', 'No Caption')\n",
    "            column_datatype = column.get('datatype', 'No Datatype')\n",
    "            column_aggregation = column.get('aggregation', 'No Aggregation')\n",
    "            column_role = column.get('role', 'No Role')\n",
    "            column_formula = column.find(\".//calculation\").get('formula', 'No Formula') if column.find(\".//calculation\") is not None else 'No Formula'\n",
    "            \n",
    "            # Print the column details\n",
    "            #print(f\"    Column Name: {column_name}, Caption: {column_caption}, Datatype: {column_datatype}, Aggregation: {column_aggregation}, Role: {column_role}\")\n",
    "            #print(f\"    Formula: {column_formula}\")\n",
    "            \n",
    "            # Prepare data for dataframe\n",
    "            data = {\n",
    "                'type': 'dashboard',\n",
    "                'title': dashboard_name,\n",
    "                'datasource': caption,\n",
    "                'column_name': column_name,\n",
    "                'formula': column_formula\n",
    "            }\n",
    "            df_temp1 = pd.DataFrame([data])\n",
    "            df_dashoard_hold.append(df_temp1)\n",
    "        \n",
    "        # Find column instances for the current datasource\n",
    "        column_instances = dashboard.findall(f\".//datasource-dependencies[@datasource='{name}']/column-instance\")\n",
    "\n",
    "        # Handle column instances\n",
    "        for column_instance in column_instances:\n",
    "            # Get column-instance attributes\n",
    "            column_instance_name = column_instance.get('name', 'No Name')\n",
    "            column_instance_base_column = re.sub(r'\\]|\\[', '', column_instance.get('column', 'No Base Column'))\n",
    "            column_instance_derivation = column_instance.get('derivation', 'No Derivation')\n",
    "            column_instance_type = column_instance.get('type', 'No Type')\n",
    "\n",
    "            # Print the column-instance details\n",
    "            #print(f\"    Column Instance: {column_instance_name}, Base Column: {column_instance_base_column}, Derivation: {column_instance_derivation}, Type: {column_instance_type}\")\n",
    "            \n",
    "            # Prepare data for dataframe\n",
    "            data = {\n",
    "                'type': 'dashboard',\n",
    "                'title': dashboard_name,\n",
    "                'datasource': caption,\n",
    "                'column_name': column_instance_name,\n",
    "                'column_instance_base_column':column_instance_base_column\n",
    "            }\n",
    "            df_temp2 = pd.DataFrame([data])\n",
    "            df_dashoard_hold.append(df_temp2)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "df_dashboard_concat = pd.concat(df_dashoard_hold).reset_index(drop=True)\n",
    "df_dashboard_concat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"306-get-worksheets-in-dashboard\">3.06 Get Worksheets in Dashboard</h2> <a id=\"306-get-worksheets-in-dashboard\"></a> \n",
    "\n",
    "##### Get worksheet names associated with a dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the worksheet names associated with a dashboard\n",
    "def get_dashboard_worksheets(dashboard):\n",
    "    # Find all 'zone' elements with the 'name' attribute (assumed to reference worksheets)\n",
    "    worksheet_references = dashboard.findall(\".//zone[@name]\")\n",
    "    worksheet_names = [zone.get('name') for zone in worksheet_references if zone.get('name')]\n",
    "    \n",
    "    # Convert to a set to remove duplicates, then back to a list\n",
    "    unique_worksheet_names = list(set(worksheet_names))\n",
    "    return unique_worksheet_names\n",
    "\n",
    "# Find all dashboard elements\n",
    "dashboards = root.findall(\".//dashboard\")\n",
    "\n",
    "df_dashboard_hold = []\n",
    "\n",
    "for dashboard in dashboards:\n",
    "    # Extract the dashboard name\n",
    "    dashboard_name = dashboard.get('name')\n",
    "    #print(f\"Dashboard: {dashboard_name}\")\n",
    "    \n",
    "    # Extract the list of unique worksheets used in the dashboard\n",
    "    worksheets_in_dashboard = get_dashboard_worksheets(dashboard)\n",
    "    \n",
    "    # Find all 'datasource' elements within the 'datasources' tag under each dashboard\n",
    "    datasources = dashboard.findall(\".//datasources/datasource\")\n",
    "    \n",
    "    for datasource in datasources:\n",
    "        # Get the 'caption' or 'name' attribute of the datasource\n",
    "        caption = datasource.get('caption', 'No Caption')  # Use a default if 'caption' doesn't exist\n",
    "        name = datasource.get('name', 'No Name')  # Use a default if 'name' doesn't exist\n",
    "        #print(f\"  Datasource: Caption={caption}, Name={name}\")\n",
    "        \n",
    "        # Find columns in the datasource-dependencies for the current datasource\n",
    "        datasource_dependencies = dashboard.findall(f\".//datasource-dependencies[@datasource='{name}']/column\")\n",
    "\n",
    "        # Handle columns\n",
    "        for column in datasource_dependencies:\n",
    "            # Get column attributes\n",
    "            column_name = re.sub(r'\\]|\\[', '', column.get('name', 'No Name'))\n",
    "            column_caption = column.get('caption', 'No Caption')\n",
    "            column_datatype = column.get('datatype', 'No Datatype')\n",
    "            column_aggregation = column.get('aggregation', 'No Aggregation')\n",
    "            column_role = column.get('role', 'No Role')\n",
    "            column_formula = column.find(\".//calculation\").get('formula', 'No Formula') if column.find(\".//calculation\") is not None else 'No Formula'\n",
    "            \n",
    "            # Print the column details\n",
    "            #print(f\"    Column Name: {column_name}, Caption: {column_caption}, Datatype: {column_datatype}, Aggregation: {column_aggregation}, Role: {column_role}\")\n",
    "            #print(f\"    Formula: {column_formula}\")\n",
    "            \n",
    "            # Prepare data for dataframe\n",
    "            data = {\n",
    "                'type': 'dashboard',\n",
    "                'title': dashboard_name,\n",
    "                'datasource': caption,\n",
    "                'column_name': column_caption,\n",
    "                'formula': column_formula,\n",
    "                'worksheets_used': worksheets_in_dashboard  # Keep worksheets as a list\n",
    "            }\n",
    "            df_temp1 = pd.DataFrame([data])\n",
    "            df_dashboard_hold.append(df_temp1)\n",
    "        \n",
    "        # Find column instances for the current datasource\n",
    "        column_instances = dashboard.findall(f\".//datasource-dependencies[@datasource='{name}']/column-instance\")\n",
    "\n",
    "        # Handle column instances\n",
    "        for column_instance in column_instances:\n",
    "            # Get column-instance attributes\n",
    "            column_instance_name = column_instance.get('name', 'No Name')\n",
    "            column_instance_base_column = re.sub(r'\\]|\\[', '', column_instance.get('column', 'No Base Column'))\n",
    "            column_instance_derivation = column_instance.get('derivation', 'No Derivation')\n",
    "            column_instance_type = column_instance.get('type', 'No Type')\n",
    "\n",
    "            # Print the column-instance details\n",
    "            #print(f\"    Column Instance: {column_instance_name}, Base Column: {column_instance_base_column}, Derivation: {column_instance_derivation}, Type: {column_instance_type}\")\n",
    "            \n",
    "            # Prepare data for dataframe\n",
    "            data = {\n",
    "                'type': 'dashboard',\n",
    "                'title': dashboard_name,\n",
    "                'datasource': caption,\n",
    "                'column_name': column_instance_base_column,\n",
    "                'worksheets_used': worksheets_in_dashboard  # Keep worksheets as a list\n",
    "            }\n",
    "            df_temp2 = pd.DataFrame([data])\n",
    "            df_dashboard_hold.append(df_temp2)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "dashboards_with_worksheets_df = pd.concat(df_dashboard_hold).reset_index(drop=True)\n",
    "dashboards_with_worksheets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"307-get-xml\">3.07 Get XML</h2> <a id=\"307-get-xml\"></a>\n",
    "\n",
    "##### Get the xml data for each dashboard, worksheet and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_hold = []\n",
    "\n",
    "# Function to convert an element and its children to a string representation of the XML\n",
    "def get_xml_string(element):\n",
    "    return etree.tostring(element, pretty_print=True, encoding='unicode')\n",
    "\n",
    "# Process all worksheets\n",
    "worksheets = root.findall(\".//worksheet\")\n",
    "for worksheet in worksheets:\n",
    "    # Extract the title and XML for each worksheet\n",
    "    worksheet_name = worksheet.get('name')\n",
    "    worksheet_xml = get_xml_string(worksheet)\n",
    "    \n",
    "    # Append to dataframe list\n",
    "    data = {\n",
    "        'title': worksheet_name,\n",
    "        'xml': worksheet_xml,\n",
    "        'type': 'worksheet'\n",
    "    }\n",
    "    df_hold.append(pd.DataFrame([data]))\n",
    "\n",
    "# Process all dashboards\n",
    "dashboards = root.findall(\".//dashboard\")\n",
    "for dashboard in dashboards:\n",
    "    # Extract the title and XML for each dashboard\n",
    "    dashboard_name = dashboard.get('name')\n",
    "    dashboard_xml = get_xml_string(dashboard)\n",
    "    \n",
    "    # Append to dataframe list\n",
    "    data = {\n",
    "        'title': dashboard_name,\n",
    "        'xml': dashboard_xml,\n",
    "        'type': 'dashboard'\n",
    "    }\n",
    "    df_hold.append(pd.DataFrame([data]))\n",
    "\n",
    "\n",
    "\n",
    "# Find all action elements in the XML\n",
    "actions = root.findall(\".//actions/action\")\n",
    "for action in actions:\n",
    "    # Extract the source dashboard value\n",
    "    source_element = action.find(\".//source\")\n",
    "    source_dashboard = source_element.get('dashboard', 'Unknown Dashboard') if source_element is not None else 'Unknown Dashboard'\n",
    "    action_xml = get_xml_string(action)\n",
    "\n",
    "    # Append to dataframe list\n",
    "    data = {\n",
    "        'title': source_dashboard,\n",
    "        'xml': action_xml,\n",
    "        'type': 'action'\n",
    "    }\n",
    "    df_hold.append(pd.DataFrame([data]))\n",
    "\n",
    "\n",
    "# Concatenate all the dataframes into a final result\n",
    "xml_with_products = pd.concat(df_hold).reset_index(drop=True)\n",
    "\n",
    "# Display the final dataframe\n",
    "xml_with_products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"308-helper-functions\">3.08 Helper Functions</h2> <a id=\"308-helper-functions\"></a>\n",
    "\n",
    "##### For processing support so you can see how things happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def xml_element_parsing(xml_data, max_token=10000, element_type=None, title=None):\n",
    "    \"\"\"\n",
    "    Parses an XML element tree, traversing through its structure, and extracts sections of XML that contain fewer than the specified token threshold.\n",
    "\n",
    "    The function recursively traverses the provided XML string, evaluates the token count for each element, and collects XML sections that meet the token count criteria. \n",
    "    If an element exceeds the token threshold, it continues to parse its child elements until it reaches the smallest section with fewer than the specified token threshold. \n",
    "    The function builds a DataFrame containing the extracted sections.\n",
    "\n",
    "    Parameters:\n",
    "    xml_data (str): A string containing the XML data to be parsed.\n",
    "    max_token (int, optional): The maximum number of tokens allowed for each XML section. Defaults to 10,000.\n",
    "    element_type (str, optional): The type of the XML element. Defaults to None.\n",
    "    title (str, optional): The title to be used if the element type is 'action'. Defaults to None.\n",
    "\n",
    "    Parameters:\n",
    "    max_token (int, optional): The maximum number of tokens allowed for each XML section. Defaults to 10,000.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the parsed XML sections, containing the following columns:\n",
    "        - 'title': The name or title of the element being parsed.\n",
    "        - 'xml': The XML content of the parsed section.\n",
    "        - 'xml_element': The XML path to the element (e.g., 'worksheet/table/view').\n",
    "        - 'type': The type of the XML element being parsed (e.g., 'worksheet').\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the input is not a string or if no sections were found to parse.\n",
    "    \"\"\"\n",
    "\n",
    "    df_hold = []\n",
    "\n",
    "    # If the input is a string, parse it as XML\n",
    "    if isinstance(xml_data, str):\n",
    "        root = etree.fromstring(xml_data)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a string containing XML data.\")\n",
    "\n",
    "    # Function to convert an element and its children to a string representation of the XML\n",
    "    def get_xml_string(element):\n",
    "        return etree.tostring(element, pretty_print=True, encoding='unicode')\n",
    "\n",
    "    # Recursive function to parse XML sections based on token count threshold\n",
    "    def parse_element(element, xml_path, title):\n",
    "        element_xml = get_xml_string(element)\n",
    "        token_count = num_tokens_from_string(element_xml, \"cl100k_base\")\n",
    "\n",
    "        if token_count < max_token:\n",
    "            data = {\n",
    "                'title': title,\n",
    "                'xml': element_xml,\n",
    "                'xml_element': xml_path,\n",
    "                'type': element.tag,\n",
    "                'token_count':token_count\n",
    "            }\n",
    "            df_hold.append(data)\n",
    "        else:\n",
    "            # If token count exceeds threshold, go deeper into child elements\n",
    "            for child in element:\n",
    "                child_path = f\"{xml_path}/{child.tag}\"\n",
    "                parse_element(child, child_path, title)\n",
    "\n",
    "    # Start parsing from the root element\n",
    "    element_name = title if element_type == 'action' and title else root.get('name', root.tag)\n",
    "    parse_element(root, root.tag, element_name)\n",
    "\n",
    "    # Create a dataframe from the collected data\n",
    "    if df_hold:\n",
    "        xml_with_products = pd.DataFrame(df_hold).reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError(\"No sections found to parse.\")\n",
    "\n",
    "    return xml_with_products\n",
    "\n",
    "\n",
    "\n",
    "def genAI_summarize_worksheet(text, api_key, model):\n",
    "    # API endpoint URL\n",
    "    api_url = 'https://api.openai.com/v1/chat/completions'\n",
    "\n",
    "    # Request headers\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': [{'role': 'system', 'content': '''\n",
    "    You are a data analyst writing documentation to summarize a Tableau worksheet. \n",
    "    Your writing style should be able to be understood by senior leadership executives along with operational teams'''},\n",
    "                        {'role': 'user', 'content': f'''\n",
    "    You are going to be provided parsed XML data from a Tableau worksheet or summarize parts of the XML in that worksheet which contain information critical to translating this metadata into \n",
    "    information about the worksheets which include the data sources and fields used. Your summariztion should be user friendly descriptive that provides details to \n",
    "    end users. \n",
    "\n",
    "    To ensure the required details are provided you must include the following:\n",
    "    1) The title of worksheet.\n",
    "    2) Datasource and fields, if available \n",
    "    3) Any available filters or other controls.\n",
    "    3) Any calculated fields and how they are derived.\n",
    "    4) Other details you may find interesting or helpful. \n",
    "                        \n",
    "    Start Parsed XML Data:\n",
    "    {text}           \n",
    "    '''}]\n",
    "    }\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.post(api_url, json=data, headers=headers)\n",
    "\n",
    "    # Process the API response\n",
    "    if response.status_code == 200:\n",
    "        print('success')\n",
    "        result = response.json()\n",
    "        chatgpt_response = result['choices'][0]['message']['content']\n",
    "\n",
    "    else:\n",
    "        print('error')\n",
    "        chatgpt_response = f\"'Error:', {response.status_code}, {response.text}\"\n",
    "\n",
    "    return chatgpt_response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def genAI_summarize_worksheet_elements(text, api_key, model):\n",
    "    # API endpoint URL\n",
    "    api_url = 'https://api.openai.com/v1/chat/completions'\n",
    "\n",
    "    # Request headers\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': [{'role': 'system', 'content': '''\n",
    "    You are a data analyst writing documentation to summarize what particular XML elements do and why they are important in a Tableau worksheet. \n",
    "    Your writing style should be able to be understood by senior leadership executives along with operational teams'''},\n",
    "                        {'role': 'user', 'content': f'''\n",
    "    You are going to be provided parsed XML data from particular XML elements in a Tableau worksheet which contain information critical to translating this metadata into \n",
    "    information about the worksheets which include the data sources and fields used. \n",
    "    Your goal is to provide a summariztion that will be added to other summarized sections that will be then be used to summarize the entire workbook.  \n",
    "\n",
    "    To ensure the required details are provided you must include the following:\n",
    "    1) The title of worksheet.\n",
    "    2) Datasource and fields, if available \n",
    "    3) Any available filters or other controls.\n",
    "    3) Any calculated fields and how they are derived.\n",
    "    4) Other details you may find interesting or helpful. \n",
    "                        \n",
    "    Start Parsed XML Data:\n",
    "    {text}           \n",
    "    '''}]\n",
    "    }\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.post(api_url, json=data, headers=headers)\n",
    "\n",
    "    # Process the API response\n",
    "    if response.status_code == 200:\n",
    "        print('success')\n",
    "        result = response.json()\n",
    "        chatgpt_response = result['choices'][0]['message']['content']\n",
    "\n",
    "    else:\n",
    "        print('error')\n",
    "        chatgpt_response = f\"'Error:', {response.status_code}, {response.text}\"\n",
    "\n",
    "    return chatgpt_response\n",
    "\n",
    "\n",
    "\n",
    "def genAI_summarize_view(text, api_key, model):\n",
    "    # API endpoint URL\n",
    "    api_url = 'https://api.openai.com/v1/chat/completions'\n",
    "\n",
    "    # Request headers\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'messages': [{'role': 'system', 'content': '''\n",
    "    You are a data analyst writing documentation to summarize a Tableau view which could be a dashboard or a worksheet. \n",
    "    Your writing style should be able to be understood by senior leadership executives along with operational teams'''},\n",
    "                        {'role': 'user', 'content': f'''\n",
    "    You are going to be provided parsed XML data from particular Tableau view which could be a dashboard or a worksheet.\n",
    "    If the view is a dashboard you will also be provided with summarized content for the worksheets that support this dashboard which need to be incorported into your response.   \n",
    "    Your goal is to provide a a complete summarization of the view that meets the following conditions.  \n",
    "\n",
    "    To ensure the required details are provided you must include the following:\n",
    "    1) The title of worksheet.\n",
    "    2) Datasource and fields, if available \n",
    "    3) Any available filters or other controls.\n",
    "    3) Any calculated fields and how they are derived.\n",
    "    4) Other details you may find interesting or helpful. \n",
    "                        \n",
    "    Start Parsed XML Data:\n",
    "    {text}           \n",
    "    '''}]\n",
    "    }\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.post(api_url, json=data, headers=headers)\n",
    "\n",
    "    # Process the API response\n",
    "    if response.status_code == 200:\n",
    "        print('success')\n",
    "        result = response.json()\n",
    "        chatgpt_response = result['choices'][0]['message']['content']\n",
    "\n",
    "    else:\n",
    "        print('error')\n",
    "        chatgpt_response = f\"'Error:', {response.status_code}, {response.text}\"\n",
    "\n",
    "    return chatgpt_response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"309-preprocessing\">3.09 Preprocessing</h2> <a id=\"309-preprocessing\"></a>\n",
    "\n",
    "##### Preprocess the dashboards, worksheets and actions to ensure the target amount of token in the XML are ready for AI processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the xml meta from the published views, only dashboard and worksheets, and determine the tokens in that view\n",
    "views_xml_records = xml_with_products[(xml_with_products.title.isin(find_views)) & (xml_with_products.type != 'action')]\n",
    "\n",
    "\n",
    "# get the actions that are scoped to the views \n",
    "view_actions = xml_with_products[(xml_with_products.title.isin(find_views)) & (xml_with_products.type == 'action')].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# get a list of the views that are dashboards and use them as a filter to get a unique listing of worksheets\n",
    "dashboard_views_xml_records = views_xml_records[views_xml_records.type == 'dashboard']\n",
    "dashboard_list = list(set(list(dashboard_views_xml_records.title)))\n",
    "\n",
    "\n",
    "# get a unique listing based on the dashboard title so a unique listing of the worksheets_used can be derived\n",
    "worksheets_in_views = dashboards_with_worksheets_df[dashboards_with_worksheets_df.title.isin(dashboard_list)].drop_duplicates(subset=('title'), keep='first')\n",
    "\n",
    "worksheet_big_list = list(worksheets_in_views.worksheets_used)\n",
    "\n",
    "# loop through each list pull the items and make one large list\n",
    "worksheet_hold_list = []\n",
    "for sheet_list in worksheet_big_list:\n",
    "    for sheet in sheet_list:\n",
    "        #print(sheet)\n",
    "        worksheet_hold_list.append(sheet)\n",
    "        \n",
    "final_worksheet_list = list(set(worksheet_hold_list))\n",
    "\n",
    "# get a listing of the worksheets that are used in the dashboards and determine their token count\n",
    "worksheet_xml_records = xml_with_products[xml_with_products.title.isin(final_worksheet_list)]\n",
    "\n",
    "\n",
    "\n",
    "# loop through views, actions and worksheets and parse the required XML\n",
    "views_xml_records_hold = []\n",
    "for view_row in views_xml_records.itertuples():\n",
    "    views_xml_records_hold.append(xml_element_parsing(view_row.xml))\n",
    "\n",
    "views_xml_records_token_processed = pd.concat(views_xml_records_hold).drop_duplicates(subset={'title','xml','type'}, keep='first').reset_index(drop=True)\n",
    "\n",
    "\n",
    "view_actions_hold = []\n",
    "for actions_row in view_actions.itertuples():\n",
    "    view_actions_hold.append(xml_element_parsing(actions_row.xml, max_token=10000, element_type='action', title=actions_row.title))\n",
    "\n",
    "view_actions_token_processed = pd.concat(view_actions_hold).drop_duplicates(subset={'title','xml','type'}, keep='first').reset_index(drop=True)\n",
    "\n",
    "\n",
    "worksheet_xml_records_hold = []\n",
    "for worksheet_row in worksheet_xml_records.itertuples():\n",
    "    worksheet_xml_records_hold.append(xml_element_parsing(worksheet_row.xml))\n",
    "\n",
    "worksheet_xml_records_token_processed = pd.concat(worksheet_xml_records_hold).drop_duplicates(subset={'title','xml','type'}, keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"310-ai-preprocessing\">3.10 AI Processing</h2> <a id=\"310-ai-preprocessing\"></a>\n",
    "\n",
    "1. ##### The code is setup for testing to save money on AI API calls so read the comments in the code to update if you want to run without restrictions. \n",
    "2. ##### The first code block processes worksheets while the next processes dashboards to derive the final product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "### comment out scoped_test_worksheets_list and replace with the variable unique_worksheet_list below when not testing to save $$$ on API calls \n",
    "#####\n",
    "# select one dashboard to process with all required worksheets\n",
    "scoped_test_worksheets_list = dashboards_with_worksheets_df[dashboards_with_worksheets_df.title == views_xml_records_token_processed.iloc[3].title].iloc[0].worksheets_used\n",
    "scoped_test_worksheets_list = [item.strip() for item in scoped_test_worksheets_list]\n",
    "\n",
    "\n",
    "# determine if there are any worksheets as views and add them to the list of worksheets for AI processing \n",
    "worksheet_views_xml_records = views_xml_records_token_processed[views_xml_records_token_processed.type == 'worksheet']\n",
    "\n",
    "if len(worksheet_views_xml_records):\n",
    "    worksheet_xml_records_token_processed = pd.concat([worksheet_xml_records_token_processed,worksheet_views_xml_records]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# remove the xml_element value 'worksheet/simple-id' from the worksheets data as this add no value for users\n",
    "worksheet_xml_records_token_processed =  worksheet_xml_records_token_processed[worksheet_xml_records_token_processed.xml_element != 'worksheet/simple-id'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# get a unique list of worksheets to iterage for AI processing\n",
    "unique_worksheet_list = list(set(list(worksheet_xml_records_token_processed.title.str.strip())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ai_df_hold = []\n",
    "\n",
    "# iterate over each worksheet and if the total amount of tokens is more than the target then process in chunks\n",
    "for sheet in scoped_test_worksheets_list:#unique_worksheet_list:  <---------------------------------------------------- change the variable to unique_worksheet_list when fully running\n",
    "\n",
    "    # get the target worksheet\n",
    "    worksheet_target = worksheet_xml_records_token_processed[worksheet_xml_records_token_processed.title.str.strip() == sheet]\n",
    "    \n",
    "    # chunk out processing if above target token count\n",
    "    if worksheet_target.token_count.sum() >= 10000:\n",
    "\n",
    "        # Grouping by xml_element and summing token_count\n",
    "        grouped_df = worksheet_target.groupby('xml_element', as_index=False)['token_count'].sum()\n",
    "\n",
    "        # Adding a new field 'processing_group' to group records by cumulative token count\n",
    "        # Goal of this is to group xml_element under the target token around while also \n",
    "        # identifing xml_element that are above so they can be parsed seperately \n",
    "        process_group = 0\n",
    "        running_total = 0\n",
    "        process_groups = []\n",
    "\n",
    "        for _, row in grouped_df.iterrows():\n",
    "            if running_total + row['token_count'] >= 10000:\n",
    "                process_group += 1\n",
    "                running_total = 0\n",
    "            running_total += row['token_count']\n",
    "            process_groups.append(process_group)\n",
    "\n",
    "        grouped_df['processing_group'] = process_groups\n",
    "\n",
    "        # get a list of the processing groups to pair accordingly \n",
    "        processing_list = list(set(list(grouped_df.processing_group)))\n",
    "\n",
    "        xml_ai_content = f'''\n",
    "        Worksheet Title: {sheet}\n",
    "        Worksheet Summarization:\n",
    "        '''\n",
    "\n",
    "        for g in processing_list:\n",
    "            process_rows = grouped_df[grouped_df.processing_group == g]\n",
    "\n",
    "            # if there is only 1 row then this indictes the xml_element\tgroup \n",
    "            # is over the target token amount and needs to have the subsections summarized\n",
    "            if len(process_rows) == 1 and process_rows.iloc[0].token_count >= 10000:\n",
    "                #\n",
    "                print(f'{process_rows.iloc[0].xml_element} <-- element above 10k for processing')\n",
    "                print(f'{worksheet_target.iloc[0].title} <-- worksheet name')\n",
    "                print('')\n",
    "                element_target_rows = worksheet_target[worksheet_target.xml_element == process_rows.iloc[0].xml_element] \n",
    "                \n",
    "                # Adding a new field 'processing_group' to group records by cumulative token count\n",
    "                process_group = 0\n",
    "                running_total = 0\n",
    "                process_groups = []\n",
    "\n",
    "                for _, row in element_target_rows.iterrows():\n",
    "                    if running_total + row['token_count'] >= 10000:\n",
    "                        process_group += 1\n",
    "                        running_total = 0\n",
    "                    running_total += row['token_count']\n",
    "                    process_groups.append(process_group)\n",
    "\n",
    "                element_target_rows['processing_group'] = process_groups           \n",
    "                \n",
    "                element_processing_list = list(set(list(element_target_rows.processing_group)))\n",
    "\n",
    "                # get the processing group rows as a set\n",
    "                for e in element_processing_list:\n",
    "                    e_target_rows = element_target_rows[element_target_rows.processing_group == e] \n",
    "\n",
    "                    # create a processing group string then send to AI to summerize\n",
    "                    xml_group_ai_content = f'''\n",
    "                    Worksheet Title: {worksheet_target.iloc[0].title}\n",
    "                    XML Element Content:\n",
    "                    '''\n",
    "\n",
    "                    # iterate over the process group rows and add the xml to the ai content\n",
    "                    for e_row in e_target_rows.itertuples():\n",
    "                        xml_group_ai_content = xml_group_ai_content + e_row.xml\n",
    "\n",
    "                    xml_ai_content = xml_ai_content + genAI_summarize_worksheet_elements(xml_group_ai_content, api_key, model)\n",
    "\n",
    "            else:\n",
    "                # use the xml_element field in the grouped_df to create a list to filter the \n",
    "                # the required rows to use the XML from those row to build the AI prompt input\n",
    "                xml_element_list = list(set(list(process_rows.xml_element)))\n",
    "\n",
    "                xml_rows = worksheet_target[worksheet_target.xml_element.isin(xml_element_list)]\n",
    "\n",
    "                # create a processing group string then send to AI to summerize\n",
    "                xml_group_ai_content = f'''\n",
    "                Worksheet Title: {worksheet_target.iloc[0].title}\n",
    "                XML Element Content:\n",
    "                '''\n",
    "\n",
    "                for x_row in xml_rows.itertuples():\n",
    "                    xml_group_ai_content = xml_group_ai_content + x_row.xml\n",
    "\n",
    "                xml_ai_content = xml_ai_content + genAI_summarize_worksheet_elements(xml_group_ai_content, api_key, model)\n",
    "\n",
    "        # get ai content and add to a dataframe for downstream processing\n",
    "        final_sum_worksheet = genAI_summarize_worksheet(xml_ai_content, api_key, model)\n",
    "        ai_data = {'title':sheet,\n",
    "                   'ai_summary':final_sum_worksheet }\n",
    "        \n",
    "        ai_temp = pd.DataFrame([ai_data])\n",
    "        ai_df_hold.append(ai_temp)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # create a processing group string then send to AI to summerize\n",
    "        xml_group_ai_content = f'''\n",
    "        Worksheet Title: {sheet}\n",
    "        XML Element Content:\n",
    "        '''\n",
    "        for w_row in worksheet_target.itertuples():\n",
    "            xml_group_ai_content = xml_group_ai_content + w_row.xml\n",
    "\n",
    "\n",
    "        # get ai content and add to a dataframe for downstream processing\n",
    "        final_sum_worksheet = genAI_summarize_worksheet(xml_group_ai_content, api_key, model)\n",
    "        ai_data = {'title':sheet,\n",
    "                   'ai_summary':final_sum_worksheet }\n",
    "\n",
    "        ai_temp = pd.DataFrame([ai_data])\n",
    "        ai_df_hold.append(ai_temp)\n",
    "\n",
    "\n",
    "worksheet_ai_content = pd.concat(ai_df_hold).reset_index(drop=True)\n",
    " \n",
    "print(worksheet_ai_content.iloc[0].ai_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Action**: Pay attention to the code comments for instructions regarding testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one dashboard to process with all required worksheets that have already been ai processed \n",
    "# After testing, uncomment view_to_process and scoped_test_worksheets_list and comment out the following two lines as these are only using one dashboard for testing\n",
    "view_to_process = views_xml_records_token_processed[views_xml_records_token_processed.title == views_xml_records_token_processed.iloc[3].title]\n",
    "scoped_test_worksheets_list = dashboards_with_worksheets_df[dashboards_with_worksheets_df.title == views_xml_records_token_processed.iloc[3].title].iloc[0].worksheets_used\n",
    "\n",
    "#### -------->>>>  UNCOMMENT the text below after testing   <<<<-------- \n",
    "#view_to_process = views_xml_records_token_processed.copy()\n",
    "#scoped_test_worksheets_list = dashboards_with_worksheets_df.copy()\n",
    "\n",
    "\n",
    "scoped_test_worksheets_list = [item.strip() for item in scoped_test_worksheets_list]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "view_ai_prep_hold = []\n",
    "\n",
    "for view in view_to_process.itertuples():\n",
    "\n",
    "    \n",
    "    \n",
    "    # if the record is a dashboard, then include the worksheets in the build process\n",
    "    view_type = view.type\n",
    "\n",
    "    ai_content = f'''\n",
    "Tableau Workbook Name: {source_workbook['Workbook Name']}, \n",
    "Tableau Workbook URL:  {source_workbook['Workbook Webpage_Url']}, \n",
    "Workbook View Name:    {view.title},\n",
    "Workbook View Type:    {view.type},\n",
    "{view.type} XML:       {view.xml}\n",
    "Included Worksheet(s) AI Summarization:        \n",
    "'''\n",
    "\n",
    "\n",
    "    if view_type == 'dashboard':\n",
    "        # get the worksheets in the view\n",
    "        worksheets_in_views = dashboards_with_worksheets_df[dashboards_with_worksheets_df.title == view.title].iloc[0].worksheets_used\n",
    "        worksheets_in_views_list = [item.strip() for item in worksheets_in_views]\n",
    "\n",
    "        \n",
    "        ai_worksheet_rows = worksheet_ai_content[worksheet_ai_content.title.isin(worksheets_in_views_list)]\n",
    "        # iterate over the worksheets and add the data to the ai_content string\n",
    "        for row in ai_worksheet_rows.itertuples():\n",
    "            # create worksheet content and add to the ai string\n",
    "            worksheet_content = f'''\n",
    "Worksheet Name: {row.title},\n",
    "Worksheet AI Summarization:  {row.ai_summary}\n",
    "'''\n",
    "            ai_content = ai_content+worksheet_content\n",
    "\n",
    "    # get ai content and add to a dataframe for downstream processing\n",
    "    final_sum_view = genAI_summarize_view(ai_content, api_key, model)\n",
    "    ai_data = {'title':view.title,\n",
    "                'ai_summary':final_sum_view}\n",
    "    \n",
    "    ai_temp = pd.DataFrame([ai_data])\n",
    "    view_ai_prep_hold.append(ai_temp)    \n",
    "\n",
    "\n",
    "      \n",
    "view_ai_preprocessed = pd.concat(view_ai_prep_hold).reset_index(drop=True)\n",
    "print(view_ai_preprocessed.iloc[0].ai_summary)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
